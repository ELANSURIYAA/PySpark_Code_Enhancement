-=============================================
- Author: AAVA
- Date: 
- Description: Refactor PySpark job for improved readability and data validation
-=============================================

# Technical Specification for Refactor PySpark job for improved readability and data validation

## 1. Introduction
* Objective: Refactor the existing PySpark job to improve configuration management, readability, and debuggability, without changing business logic or output.
* Scope: Parameterize all hard-coded values (paths, constants), add row-count validation and logging after key transformation steps, and ensure no changes to business logic or output.

## 2. Code Changes Required
* Impacted Modules:
    - Main PySpark job script responsible for ETL processing.
    - Configuration management (where input/output paths and constants are set).
    - Logging mechanism for row counts.
* Logic Implementation:
    1. Identify all hard-coded file paths, constants, and environment-specific values in the PySpark job.
    2. Replace hard-coded values with parameterized variables (e.g., via config file, command-line args, or environment variables).
    3. After each major filtering or transformation step, insert logic to:
        - Calculate row count (e.g., `df.count()`)
        - Log row count with a descriptive message (e.g., using `logger.info`)
    4. Ensure the refactored job produces identical output to the original.
    5. Validate job execution for successful end-to-end run with no performance regression.

## 3. Data Model Updates
* New/Modified Objects:
    - None. No schema or data model changes per scope.
* Schema Definition:
    - N/A (No changes required)

## 4. Source-to-Target Mapping
| Source Field              | Target Field              | Transformation Rule                  |
|--------------------------|--------------------------|--------------------------------------|
| Hard-coded input path     | Parameterized input path | Use config/parameter instead of code |
| Hard-coded output path    | Parameterized output path| Use config/parameter instead of code |
| Hard-coded constants      | Parameterized constants  | Use config/parameter instead of code |
| DataFrame (pre-transform)| DataFrame (post-transform)| Log row count before/after step      |

## 5. Assumptions and Constraints
* Assumptions:
    - All hard-coded values are easily identifiable in the current PySpark job.
    - Logging framework (e.g., Python `logging` or Spark log4j) is available and can be used for row count logging.
    - No changes to business logic, schema, or output format are required.
    - Parameterization can be achieved via config file, environment variables, or command-line arguments.
* Constraints:
    - No refactoring into helper functions.
    - No consolidation of column logic.
    - No caching or performance tuning.
    - No additional audit or reject outputs.

## 6. References
* JIRA ID: PCE-4

---

### Cost Estimation

| Task                                   | Effort (Person-Days) |
|---------------------------------------- |----------------------|
| Identify and parameterize hard-coded values | 0.5                  |
| Implement row-count logging                | 0.5                  |
| Testing and validation                     | 0.5                  |
| Peer review and documentation              | 0.25                 |
| **Total Estimated Effort**                 | **1.75 Person-Days** |
